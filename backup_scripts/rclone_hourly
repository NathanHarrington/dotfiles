#!/bin/bash
#
# CAUTION:
# Option --size-only is used in rclone. This will avoid
# issues with rclone copying files whose hash has changed. This also
# makes it possible to edit a single file in the auto_backup folder, 
# don't increase the file size, and this script may not backup the
# change. GPG is also changing the file size every pass, sometime it
# matches the last size, sometimes not. 
# ########
#
# Designed to be run every hour from crontab. This simply looks for a
# folder with todays date on the various services. If it does not exist
# it creates it. It will then upload the file to the folder. At the next
# pass the file will only be uploaded if changed locally. This is
# designed to perform backups within an hour of change, whenever the
# system is available.
#
# This is for your current critical backups use case: Primarily text
# files with log entries and sensitive information. The occaisional pdf.
# More data intensive operations such as pictures of the kids, work
# products and task management are all done elsewhere. This is the
# core data that you never want to lose, and is  usually about 1MB of
# new data per year. This strategy provides an hourly backup of any and
# all files placed in a directory using rclone. 
#
# StorageService:/(Backup_prefix)auto_backup/time_frame/
#
# Where StorageService is google drive, dropbox, back blaze b2, etc.
# auto_backup is a directory in the root folder, and hourly, daily,
# weekly are manually created folders. Backup_prefix is the two letter
# prefix of the use-case (nh for home, wp for wasatch photonics). 
#
# This is to create distributed backups that are other sys-admins
# responsibilities. 
#
# Add the following entries to your crontab
#SCRIPTS=/home/nharrington/projects/dotfiles/backup_scripts
#BACKUP_PREFIX=wp  (change this to the correct prefix! )
#13 * * * * $SCRIPTS/encrypt_directory.sh >>$SCRIPTS/backup.log 2>&1
#14 * * * * $SCRIPTS/rclone_hourly >>$SCRIPTS/backup.log 2>&1


# rclone remote names
declare -a dest=("${BACKUP_PREFIX}dropbox" "${BACKUP_PREFIX}gdrive"
"${BACKUP_PREFIX}backblaze" "${BACKUP_PREFIX}onedrive")
#declare -a dest=("${BACKUP_PREFIX}gdrive")

RCLONE=/usr/sbin/rclone
RCLONE_OPTS="--verbose --size-only"

HOSTNAME=$(hostname)
TAR_GPG_FILENAME="${HOSTNAME}_encrypted.tar.gpg"

SRC_FILENAME="$HOME/Documents/working_encrypted/$TAR_GPG_FILENAME"

SCRIPT_NAME=`basename "$0"`

MONTH=`date +%Y%m`
DAY=`date +%Y%m%d`

for DST_TYPE in "${dest[@]}"
do
    LOCATION="/auto_backup/${SCRIPT_NAME}"

    # Prepend the bucket name for backblaze b2
    if [ "$DST_TYPE" == "${BACKUP_PREFIX}backblaze" ]; then
        echo "Back blaze bucket prepend"
        LOCATION="${BACKUP_PREFIX}autobackupbucket${LOCATION}"
    fi

    # Perform the actual copy with rclone. rclone automatically creates
    # the folder if it does not exist.
    NEW_FILE=$DST_TYPE:$LOCATION/$MONTH/$DAY-$TAR_GPG_FILENAME

    echo "$RCLONE $RCLONE_OPTS copy $SRC_FILENAME $NEW_FILE"
    $RCLONE $RCLONE_OPTS copy $SRC_FILENAME $NEW_FILE

done

